{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of Yelp Dataset to Reviews for Restaurants in Toronto with User, Item, Rating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"farhaan/yel_data/\")\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from __future__ import division\n",
    "import time\n",
    "\n",
    "# Read review dataset\n",
    "review = pd.read_csv(\"yelp_academic_dataset_review.csv\")\n",
    "# review.shape\n",
    "\n",
    "# Count number of unqiue users and items for review dataset\n",
    "n_users_review = review.user_id.unique().shape[0]\n",
    "n_items_review = review.business_id.unique().shape[0]\n",
    "\n",
    "\n",
    "# Read business dataset\n",
    "business = pd.read_csv(\"yelp_academic_dataset_business.csv\")\n",
    "\n",
    "# Group businesses by 'city'\n",
    "city = business.groupby('city')['city'].count()\n",
    "\n",
    "# Subset business to category 'Restaurants'\n",
    "restaurant = business[business['categories'].str.contains(\"Restaurants\",na=False)]\n",
    "\n",
    "# Group restaurant businesses by city\n",
    "city2 = restaurant.groupby('city')['city'].count()\n",
    "\n",
    "# Filter restaurant businesses to city 'Toronto'\n",
    "restaurant_toronto = restaurant.loc[restaurant['city'] == 'Toronto']\n",
    "\n",
    "# Left join restaruants in toronto table with review table\n",
    "review_rest_tor = pd.merge(restaurant_toronto, review, on='business_id', how='left')\n",
    "\n",
    "# Subset to user, item, rating columns\n",
    "uir = review_rest_tor[['user_id','business_id','stars_y']]\n",
    "user_index = uir.user_id.unique()\n",
    "item_index = uir.business_id.unique()\n",
    "\n",
    "# Count number of unique users and items\n",
    "n_users = uir.user_id.unique().shape[0]\n",
    "n_items = uir.business_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create User, Item Matrix and Filter down the dataset to 5 Core with every user and item with at least 5 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split User, Item, Rating dataset to train and test sets of 70% & 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(uir, test_size=0.30, random_state=42)\n",
    "\n",
    "# Create table for train data with list of users as index & items as columns\n",
    "train_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "\n",
    "# Fill in train_matrix table with ratings\n",
    "for row in train.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    train_matrix.loc[user][item] = row[3]  \n",
    "\n",
    "# Create table for test data with list of users as index & items as columns    \n",
    "test_matrix = pd.DataFrame(index=user_index, columns=item_index)\n",
    "\n",
    "# Fill in test_matrix table with ratings\n",
    "for row in test.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    test_matrix.loc[user][item] = row[3]\n",
    "\n",
    "# Begin filtering process to create 5 Core Subset\n",
    "\n",
    "# Count number of rated items for each user\n",
    "item_1 = train_matrix.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# item_1.value_counts()\n",
    "\n",
    "# Filter down to the users with greater than or equal to 5 ratings\n",
    "train1 = train_matrix\n",
    "train1['item_1'] = item_1\n",
    "train2 = train1.loc[train1['item_1'] >= 5]\n",
    "# train2.shape\n",
    "\n",
    "# Count number of rated users for each item\n",
    "train2 = train2.drop('item_1',axis=1)\n",
    "train3 = train2.transpose()\n",
    "user_1 = train3.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "# user_1.value_counts()\n",
    "\n",
    "# Filter down to the items with greater than or equal to 5 ratings\n",
    "train3['user_1'] = user_1\n",
    "train4 = train3.loc[train3['user_1'] >= 5]\n",
    "train4 = train4.drop('user_1',axis=1)\n",
    "train5 = train4.transpose()\n",
    "\n",
    "# Repeat the process for both user and item\n",
    "item_2 = train5.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "train5['item_2'] = item_2\n",
    "train6 = train5.loc[train5['item_2'] >= 5]\n",
    "train6 = train6.drop('item_2',axis=1)\n",
    "train7 = train6.transpose()\n",
    "user_2 = train7.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "train7['user_2'] = user_2\n",
    "train8 = train7.loc[train7['user_2'] >= 5]\n",
    "train8 = train8.drop('user_2',axis=1)\n",
    "train9 = train8.transpose()\n",
    "\n",
    "# Check every user and item has at least 5 ratings\n",
    "item_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "user_3 = train9.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "# Filter down the test matrix to filtered user and item in train matrix\n",
    "test9 = test_matrix.loc[train9.index,train9.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for Restaurants in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter restaurant businesses to city 'Phoenix'\n",
    "restaurant_phoenix = restaurant.loc[restaurant['city'] == 'Phoenix']\n",
    "\n",
    "\n",
    "# Left join restaruants in phoenix table with review table\n",
    "review_rest_pho = pd.merge(restaurant_phoenix, review, on='business_id', how='left')\n",
    "\n",
    "# Subset to user, item, rating columns\n",
    "uir_ph = review_rest_pho[['user_id','business_id','stars_y']]\n",
    "\n",
    "# Assign index for user and item\n",
    "user_index_ph = uir_ph.user_id.unique()\n",
    "item_index_ph = uir_ph.business_id.unique()\n",
    "\n",
    "# Count number of unique users and items\n",
    "n_users_ph = uir_ph.user_id.unique().shape[0]\n",
    "n_items_ph = uir_ph.business_id.unique().shape[0]\n",
    "\n",
    "# Split User, Item, Rating dataset to train and test sets of 70% & 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ph, test_ph = train_test_split(uir_ph, test_size=0.30, random_state=42)\n",
    "\n",
    "# Create table for train data with list of users as index & items as columns\n",
    "train_matrix_ph = pd.DataFrame(index=user_index_ph, columns=item_index_ph)\n",
    "\n",
    "# Fill in train_matrix table with ratings\n",
    "for row in train_ph.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    train_matrix_ph.loc[user][item] = row[3]  \n",
    "\n",
    "# Create table for test data with list of users as index & items as columns    \n",
    "test_matrix_ph = pd.DataFrame(index=user_index_ph, columns=item_index_ph)\n",
    "\n",
    "# Fill in test_matrix table with ratings\n",
    "for row in test_ph.itertuples():\n",
    "    user = row[1]\n",
    "    item = row[2]\n",
    "    test_matrix_ph.loc[user][item] = row[3]\n",
    "\n",
    "\n",
    "# Count number of rated items for each user\n",
    "item_1_ph = train_matrix_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "# Filter down to the users with greater than or equal to 5 ratings\n",
    "train1_ph = train_matrix_ph\n",
    "train1_ph['item_1'] = item_1_ph\n",
    "train2_ph = train1_ph.loc[train1_ph['item_1'] >= 5]\n",
    "\n",
    "# Count number of rated users for each item\n",
    "train2_ph = train2_ph.drop('item_1',axis=1)\n",
    "train3_ph = train2_ph.transpose()\n",
    "user_1_ph = train3_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "# Filter down to the items with greater than or equal to 5 ratings\n",
    "train3_ph['user_1'] = user_1_ph\n",
    "train4_ph = train3_ph.loc[train3_ph['user_1'] >= 5]\n",
    "train4_ph = train4_ph.drop('user_1',axis=1)\n",
    "train5_ph = train4_ph.transpose()\n",
    "\n",
    "# Repeat the process for both user and item\n",
    "item_2_ph = train5_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "train5_ph['item_2'] = item_2_ph\n",
    "train6_ph = train5_ph.loc[train5_ph['item_2'] >= 5]\n",
    "train6_ph = train6_ph.drop('item_2',axis=1)\n",
    "train7_ph = train6_ph.transpose()\n",
    "user_2_ph = train7_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "train7_ph['user_2'] = user_2_ph\n",
    "train8_ph = train7_ph.loc[train7_ph['user_2'] >= 5]\n",
    "train8_ph = train8_ph.drop('user_2',axis=1)\n",
    "train9_ph = train8_ph.transpose()\n",
    "# train9_ph.shape\n",
    "\n",
    "item_3_ph = train9_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "user_3_ph = train9_ph.apply(lambda x: x > 0, raw=True).sum(axis=1)\n",
    "\n",
    "# Filter down the test matrix to filtered user and item in train matrix\n",
    "test9_ph = test_matrix_ph.loc[train9_ph.index,train9_ph.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Yelp dataset & User, Item, Rating dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yelp Review Dataset\n",
      "Size of original Yelp dataset, Review is (4153150, 10) \n",
      "Number of users is 1029432, number of items is 144072 \n",
      "\n",
      "Yelp Business Dataset\n",
      "Size of original Yelp dataset, Business is (144072, 16) \n",
      "Size of Restaurants subset of Business dataset is (48485, 16) \n",
      "\n",
      "Top 10 Cities with the most number of restaurants are as follows \n",
      "city\n",
      "Toronto        6347\n",
      "Las Vegas      5431\n",
      "Phoenix        3353\n",
      "MontrÃ©al       2852\n",
      "Charlotte      2201\n",
      "Pittsburgh     1990\n",
      "Edinburgh      1412\n",
      "Scottsdale     1356\n",
      "Cleveland      1235\n",
      "Mississauga    1128\n",
      "Name: city, dtype: int64 \n",
      "\n",
      "Restaurants in Toronto Subset\n",
      "Size of Restaurants in Toronto subset of Business dataset is (6347, 16) \n",
      "User, Item, Rating dataset for restuarnats in Toronto contain 245127 rows \n",
      "Number of users is 58355, number of items is 6347 \n",
      "\n",
      "Restaurants in Phoenix Subset\n",
      "Size of Restaurants in Phoenix subset of Business dataset is (3353, 16) \n",
      "User, Item, Rating dataset for restuarnats in Toronto contain 266766 rows \n",
      "Number of users is 97476, number of items is 3353 \n"
     ]
    }
   ],
   "source": [
    "print '\\nYelp Review Dataset'\n",
    "print 'Size of original Yelp dataset, Review is %s ' % str(review.shape)\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users_review),str(n_items_review))\n",
    "print '\\nYelp Business Dataset'\n",
    "print 'Size of original Yelp dataset, Business is %s ' % str(business.shape)\n",
    "print 'Size of Restaurants subset of Business dataset is %s ' % str(restaurant.shape)\n",
    "print '\\nTop 10 Cities with the most number of restaurants are as follows \\n%s ' % str(city2.sort_values(ascending=False).head(10))\n",
    "print '\\nRestaurants in Toronto Subset'\n",
    "print 'Size of Restaurants in Toronto subset of Business dataset is %s ' % str(restaurant_toronto.shape)\n",
    "print 'User, Item, Rating dataset for restuarnats in Toronto contain %s rows ' % str(len(uir))\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users),str(n_items))\n",
    "print '\\nRestaurants in Phoenix Subset'\n",
    "print 'Size of Restaurants in Phoenix subset of Business dataset is %s ' % str(restaurant_phoenix.shape)\n",
    "print 'User, Item, Rating dataset for restuarnats in Toronto contain %s rows ' % str(len(uir_ph))\n",
    "print 'Number of users is %s, number of items is %s ' % (str(n_users_ph),str(n_items_ph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Test Matrices of Users, Items & 5 Core Subset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restaurants in Toronto\n",
      "\n",
      "Train dataset contains 171588 rows, Test dataset contains 73539 rows \n",
      "Size of train & test matrices with users in rows & items in columns is (58355, 6348) \n",
      "\n",
      "Users of filtered 5 core subset has rated at least 5 items as follows  \n",
      "5    1419\n",
      "6     946\n",
      "7     663\n",
      "8     568\n",
      "9     436\n",
      "dtype: int64\n",
      "\n",
      "Items of filtered 5 core subset has been rated by at least 5 users as follows  \n",
      "5    1419\n",
      "6     946\n",
      "7     663\n",
      "8     568\n",
      "9     436\n",
      "dtype: int64\n",
      "\n",
      "Size of 5 core subset is (7016, 3948) \n",
      "\n",
      "Restaurants in Phoenix\n",
      "\n",
      "Train dataset contains 186736 rows, Test dataset contains 80030 rows \n",
      "Size of train & test matrices with users in rows & items in columns is (97476, 3354) \n",
      "\n",
      "Size of 5 core subset is (7032, 2153) \n"
     ]
    }
   ],
   "source": [
    "print '\\nRestaurants in Toronto'\n",
    "print '\\nTrain dataset contains %s rows, Test dataset contains %s rows ' % (str(len(train)),str(len(test)))\n",
    "print 'Size of train & test matrices with users in rows & items in columns is %s ' % (str(train_matrix.shape))\n",
    "print '\\nUsers of filtered 5 core subset has rated at least 5 items as follows  \\n%s' % (str(item_3.value_counts().head(5)))\n",
    "print '\\nItems of filtered 5 core subset has been rated by at least 5 users as follows  \\n%s' % (str(user_3.value_counts().head(5)))\n",
    "print '\\nSize of 5 core subset is %s ' % (str(train9.shape))\n",
    "print '\\nRestaurants in Phoenix'\n",
    "print '\\nTrain dataset contains %s rows, Test dataset contains %s rows ' % (str(len(train_ph)),str(len(test_ph)))\n",
    "print 'Size of train & test matrices with users in rows & items in columns is %s ' % (str(train_matrix_ph.shape))\n",
    "print '\\nSize of 5 core subset is %s ' % (str(train9_ph.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features dataset for Restaurants in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = restaurant_toronto[['business_id','attributes','categories']]\n",
    "\n",
    "# Extract features from column, categories & attributes for items(restaurants in Toronto)\n",
    "feature1 = pd.DataFrame(feature['categories'].astype(str).str.split(',').tolist())\n",
    "feature1 = feature1.apply(lambda x: x.str.lower())\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace('[',''))\n",
    "feature1 = feature1.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "# Extract features from column, attributes for items(restaurants in Toronto)\n",
    "feature2 = pd.DataFrame(feature['attributes'].astype(str).str.split(',').tolist())\n",
    "feature2 = feature2.apply(lambda x: x.str.lower())\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(r'.*: {',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('}\\\"',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(':',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace('[',''))\n",
    "feature2 = feature2.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "feature1 = feature1.as_matrix()\n",
    "feature2 = feature2.as_matrix()\n",
    "features = np.concatenate((feature1, feature2),axis=1)\n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "features.index = feature['business_id']\n",
    "filtered = list(train9.columns.values)\n",
    "features_filt = features[features.index.isin(filtered)]\n",
    "number_of_features = pd.unique(features_filt.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features dataset for Restaurants in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_ph = restaurant_phoenix[['business_id','attributes','categories']]\n",
    "\n",
    "# Extract features from column, categories & attributes for items(restaurants in Phoenix)\n",
    "feature1_ph = pd.DataFrame(feature_ph['categories'].astype(str).str.split(',').tolist())\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.lower())\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace('[',''))\n",
    "feature1_ph = feature1_ph.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "# Extract features from column, attributes for items(restaurants in Phoenix)\n",
    "feature2_ph = pd.DataFrame(feature_ph['attributes'].astype(str).str.split(',').tolist())\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.lower())\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('u\\'',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('u\\\"',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(r'.*: {',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('}\\\"',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('\\'',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(':',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace('[',''))\n",
    "feature2_ph = feature2_ph.apply(lambda x: x.str.replace(']',''))\n",
    "\n",
    "feature1_ph = feature1_ph.as_matrix()\n",
    "feature2_ph = feature2_ph.as_matrix()\n",
    "features_ph = np.concatenate((feature1_ph, feature2_ph),axis=1)\n",
    "features_ph = pd.DataFrame(features_ph)\n",
    "\n",
    "features_ph.index = feature_ph['business_id']\n",
    "filtered_ph = list(train9_ph.columns.values)\n",
    "features_filt_ph = features_ph[features_ph.index.isin(filtered_ph)]\n",
    "number_of_features_ph = pd.unique(features_filt_ph.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurants in Toronto\n",
      "\n",
      "There are 536 features\n",
      "Examples of features are as follows \n",
      "[' lunch false' ' dinner false' ' breakfast true' ' brunch true'\n",
      " ' hastv true' ' noiselevel average' ' restaurantspricerange2 1'\n",
      " ' restaurantsreservations true' ' restaurantstableservice true'\n",
      " ' wheelchairaccessible true' ' wifi free' 'restaurants' ' japanese'\n",
      " ' sushi bars' 'alcohol beer_and_wine' ' restaurantsdelivery true' 'korean'\n",
      " ' street true' ' caters false' ' goodforkids false'] \n",
      "\n",
      "Restaurants in Phoenix\n",
      "\n",
      "There are 476 features\n",
      "Examples of features are as follows \n",
      "[' kosher true' ' halal true' ' soy-free true' 'karaoke' 'latin american'\n",
      " ' cambodian' 'tapas/small plates' ' szechuan' ' local flavor'\n",
      " ' fruits & veggies' 'drugstores' 'bikeparking false' 'sports clubs'\n",
      " ' food stands' ' jazz & blues' 'caribbean' ' cuban' 'arts & entertainment'\n",
      " ' cupcakes' 'fondue'] \n"
     ]
    }
   ],
   "source": [
    "print 'Restaurants in Toronto'\n",
    "print '\\nThere are %s features' % str(len(number_of_features))\n",
    "print 'Examples of features are as follows \\n%s ' % str(number_of_features[50:70])\n",
    "print '\\nRestaurants in Phoenix'\n",
    "print '\\nThere are %s features' % str(len(number_of_features_ph))\n",
    "print 'Examples of features are as follows \\n%s ' % str(number_of_features_ph[350:370])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_based_recommender(train, test, features):\n",
    "\n",
    "    # For each user, retrieve item id's of rated items\n",
    "    col = train.columns\n",
    "    pref_known_train = train.apply(lambda x: x > 0, raw=True).apply(lambda x: list(col[x.values]), axis=1)\n",
    "\n",
    "    cols = test.columns\n",
    "    pref_known_test = test.apply(lambda x: x > 0, raw=True).apply(lambda x: list(cols[x.values]), axis=1)\n",
    "\n",
    "    mean_rating = train.mean(axis=1)\n",
    "    mean_user_rating = pd.concat([mean_rating] * len(train.columns), axis=1)\n",
    "    mean_user_rating.columns = train.columns\n",
    "    \n",
    "    test_pred = pd.DataFrame(index=test.index, columns=test.columns)\n",
    "\n",
    "    for m in range(len(train)):\n",
    "        like_m = pd.DataFrame(data=None, columns=['feature','like','dislike'])\n",
    "        like = 0\n",
    "        dislike = 0\n",
    "        total = 0\n",
    "        for x1 in pref_known_train[m]:\n",
    "            if train.loc[train.index[m]][x1] > 2:\n",
    "                like = like + 1\n",
    "                for z1 in np.array(features.loc[x1]):\n",
    "                    if z1 is not None:\n",
    "                        like_new1 = pd.DataFrame([[z1,1,0]],columns=['feature','like','dislike'])\n",
    "                        like_m = like_m.append(like_new1)\n",
    "                    \n",
    "            else:\n",
    "                dislike = dislike + 1\n",
    "                for z1 in np.array(features.loc[x1]):\n",
    "                    if z1 is not None:\n",
    "                        like_new1 = pd.DataFrame([[z1,0,1]],columns=['feature','like','dislike'])\n",
    "                        like_m = like_m.append(like_new1)\n",
    "            total = total + 1\n",
    "    \n",
    "        prob_like =  like / total\n",
    "        prob_dislike = dislike / total\n",
    "        like_m_f = like_m.groupby(\"feature\").sum()\n",
    "\n",
    "        like_m_f['pL'] = (like_m_f['like'] + 0.1) / (like + 2*(0.1))\n",
    "        like_m_f['pDL'] = (like_m_f['dislike'] + 0.1) / (dislike + 2*(0.1))\n",
    "\n",
    "        for x2 in pref_known_test[m]:\n",
    "            like_m_l = pd.DataFrame(data=None, columns=['feature'])\n",
    "            for z2 in np.array(features.loc[x2]):\n",
    "                if z2 is not None:\n",
    "                    like_new2 = pd.DataFrame([[z2]],columns=['feature'])\n",
    "                    like_m_l = like_m_l.append(like_new2)\n",
    "        \n",
    "            like_m_l_f = like_m_l.feature.unique()\n",
    "\n",
    "            for w in like_m_l_f:\n",
    "                if w not in like_m_f.index:\n",
    "                    like_new = pd.DataFrame([[0,0,0.1/(like+(2*0.1)),0.1/(dislike+(2*0.1))]],index=[w],columns=['like','dislike','pL','pDL'])\n",
    "                    like_m_f = like_m_f.append(like_new) \n",
    "        \n",
    "            pLiked = prob_like\n",
    "            for v in np.array(features.loc[x2]):\n",
    "                if v is not None:\n",
    "                    pLiked = pLiked * like_m_f.loc[str(v)]['pL']\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            pDisliked = prob_dislike\n",
    "            for v2 in np.array(features.loc[x2]):\n",
    "                if v2 is not None:\n",
    "                    pDisliked = pDisliked * like_m_f.loc[str(v2)]['pDL']\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "            P1 = pLiked / (pLiked+pDisliked)\n",
    "            P2 = pDisliked / (pLiked+pDisliked)\n",
    "            val = np.log(P1/P2)\n",
    "            rat = 1/(1+np.exp(-val))\n",
    "            rating = rat*4 +1 \n",
    "            rating\n",
    "\n",
    "            test_pred.loc[train.index[m]][x2] = rating\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based Recommender for Restaurant in Toronto took 8280.69850421 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_pred = content_based_recommender(train9, test9, features_filt)\n",
    "print 'Content Based Recommender for Restaurant in Toronto took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:99: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based Recommender for Restaurant in Phoenix took 7056.17153001 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_pred_ph = content_based_recommender(train9_ph, test9_ph, features_filt_ph)\n",
    "print 'Content Based Recommender for Restaurant in Phoenix took %s seconds' % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based Recommenders Sytems for Toronto Mean Absolute Error: 1.5151\n",
      "Content-Based Recommenders Sytems for Phoenix Mean Absolute Error: 1.4289\n"
     ]
    }
   ],
   "source": [
    "test_pred_0 = test_pred.fillna(0)\n",
    "test_pred_0_mat = test_pred_0.as_matrix()\n",
    "pred_list = list(test_pred.values.ravel())\n",
    "actual_list = list(test9.values.ravel())\n",
    "test_pred_0_ph = test_pred_ph.fillna(0)\n",
    "test_pred_0_mat_ph = test_pred_0_ph.as_matrix()\n",
    "pred_list_ph = list(test_pred_ph.values.ravel())\n",
    "actual_list_ph = list(test9_ph.values.ravel())\n",
    "print 'Content-Based Recommenders Sytems for Toronto Mean Absolute Error: ' + str(round(mae(test_pred_0_mat,test9_0_mat),4))\n",
    "print 'Content-Based Recommenders Sytems for Phoenix Mean Absolute Error: ' + str(round(mae(test_pred_0_mat_ph,test9_0_mat_ph),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
